name: "LassoNetMLP"
optimizer: "adam"
criterion: "crossentropy"
drop_layer: False
hotstart_model: False # Else give path to model
flatten: False # whether or not input needs to be flattened before passing to model
hyper:
  track_features: [0, 13, 27, 385, 397, 411, 756, 770, 783] # default False
  lr: 0.001
  epochs: 20
  batch_size: 256
  n_neurons: 256  
  n_depth: 2 # number of hidden layers (not including first and last layer)
  dropout: 0.0
  kernel_size: 3
  patience: 20
  skip_gl_lambda: 5
  lambda_multiplier: 1.02
  M: 2 # determines importance of skip connection (see Lassonet paper)
  lambda_start_bar: 0
  lambda_start_factor: 2
  lambda_start_begin: 1e-6
  check_zero_threshold: 1e-5
