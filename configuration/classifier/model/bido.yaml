# Peng et al. 2022 defense

name: "BiDO"
optimizer: "adam"
criterion: "crossentropy"
flatten: False # whether or not input needs to be flattened before passing to model
lr_scheduler: False # "MultiStepLR", 

args:
  measure: "HSIC" # HSIC or COCO
  ktype: "linear" # linear, gaussian, IMQ
  hsic_training: True
  hotstart_model: False # Else give path to model
  root_path: "Defend_MI"
  config_dir: 'config'
  model_dir: 'target_model'

loaded_args:
  dataset:
    model_name: "VGG16"
    train_file: ""
    test_file: ""
    n_classes: None # define this in dataset yaml file instead

hyper:
  lr: 0.001
  epochs: 20
  batch_size: 128
  patience: 100
  adjust_epochs: None
  momentum: 0.9
  weight_decay: 1e-4
  gamma: 0.1
  beta2: 0.999
  milestones: False # Ex. [75, 90]
  # gamma: False # Ex. 0.1, factor by which lr changes at each milestone for MultiStepLR