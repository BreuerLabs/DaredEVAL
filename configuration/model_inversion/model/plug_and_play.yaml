name: "plug_and_play"
stylegan_model: "stylegan2-ada-pytorch/ffhq.pkl"

evaluation_model:
  architecture: inception-v3 # architecture of evaluation model
  weights: trained_models/facescrub/inception_v3_facescrub.pt # link to weight file

# From the pnp repo
candidates:
  num_candidates: 10
  candidate_search:
    search_space_size: 2000
    center_crop: null
    # resize: 224 #! This is removed because it depends on dataset not on attack 
    horizontal_flip: true
    batch_size: 25
    truncation_psi: 0.5
    truncation_cutoff: 8

attack:
  batch_size: 1 # Batch size per GPU.
  num_epochs: 10 # Number of optimization iterations per batch.
  targets: 0 # Specify the targeted classes, either a single class index, a list of indices, or all.
  discriminator_loss_weight: 0.0 # Add discriminator weight.
  single_w: true # Optimize a single 512-vector. Otherwise, a distinct vector for each AdaIn operation is optimized.
  clip: false # Clip generated images in range [-1, 1].
  transformations: # Transformations applied during the optimization.
    CenterCrop:
      size: 800
    Resize:
      size: 224
      antialias: true

    RandomResizedCrop:
      size: [224, 224]
      scale: [0.9, 1.0]
      ratio: [1.0, 1.0]
      antialias: true

  optimizer: # Optimizer used for optimization. All optimizers from torch.optim are possible.
    Adam:
      lr: 0.005
      weight_decay: 0
      betas: [0.1, 0.1]

  lr_scheduler: # Option to provide a learning rate scheduler from torch.optim.
    MultiStepLR:
      milestones: [30, 40]
      gamma: 0.1

final_selection:
  samples_per_target: 50 # Number of samples to select from the set of optimized latent vectors.
  approach: transforms # Currently only transforms is available as an option.
  iterations: 100 # Number of iterations random transformations are applied.

wandb: # Options for WandB logging.
  enable_logging: false # Activate logging.
  wandb_init_args: # WandB init arguments.
    project: plug_and_play
    save_code: true


# For GAN
# label_dim: 10
# hyper:
#   epochs: 3
#   batch_size: 256
#   n_critic: 5
# discriminator:
#   lr: 0.0001
#   dropout: 0.3
#   hidden_dim: [1024, 512, 256]
#   negative_slope: 0.2 # Negative slope of the leaky ReLU function
# generator:
#   lr: 0.0001
#   latent_dim: 100
#   hidden_dim: [256, 512, 1024]
#   negative_slope: 0.2 # Negative slope of the leaky ReLU function