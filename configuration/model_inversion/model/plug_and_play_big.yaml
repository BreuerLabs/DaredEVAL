name: "plug_and_play"
stylegan_model: "stylegan2-ada-pytorch/ffhq.pkl"

evaluation_model:
  wandb_id: "r71jl57n" # The wandb id of the run of the evaluation model. This will then extract the config and model weights from wandb.
  config_path: False #"classifiers/saved_configs/cifardropout.yaml" # The path to a config of a trained evaluation model. Weights with the same name must be in classifiers/saved_models

  # architecture: inception-v3 # architecture of evaluation model
  # weights: classifiers/saved_models/soft-wood-101.pth # link to weight file

candidates:
  num_candidates: 200
  candidate_search:
    search_space_size: 2000
    center_crop: 800
    resize: 224
    horizontal_flip: true
    batch_size: 25
    truncation_psi: 0.5
    truncation_cutoff: 8

attack:
  batch_size: 25
  num_epochs: 50
  targets: all
  discriminator_loss_weight: 0.0
  single_w: true
  clip: false
  transformations:
    CenterCrop:
      size: 800
    Resize:
      size: 224
      antialias: true
    RandomResizedCrop:
      size: [224, 224]
      scale: [0.9, 1.0]
      ratio: [1.0, 1.0]
      antialias: true

  optimizer: # Optimizer used for optimization. All optimizers from torch.optim are possible.
    Adam:
      lr: 0.005
      weight_decay: 0
      betas: [0.1, 0.1]

  lr_scheduler: # Option to provide a learning rate scheduler from torch.optim.
    MultiStepLR:
      milestones: [30, 40]
      gamma: 0.1

final_selection:
  samples_per_target: 50 # Number of samples to select from the set of optimized latent vectors.
  approach: transforms # Currently only transforms is available as an option.
  iterations: 100 # Number of iterations random transformations are applied.

wandb: # Options for WandB logging.
  enable_logging: false # Activate logging.
  wandb_init_args: # WandB init arguments.
    project: plug_and_play
    save_code: true

